general:
  task: "rl"
  random_seed: 1  # 0, 1, 2, 3, 4
  use_this_many_data: -1
  use_cuda: True  # disable this when running on machine without cuda
  use_fp16: True
  tensorboard: True

  training:
    batch_size: 64
    max_episode: 300000
    smoothing_eps: 0.1
    optimizer:
      step_rule: 'radam'  # adam, radam
      learning_rate: 1.0e-3
      learning_rate_decay: 'linear'
      clip_grad_norm: 5
      learning_rate_warmup_until:  1000
    fix_parameters_keywords: []
    patience: 0  # >=1 to enable

  evaluate:
    run_eval: True
    batch_size: 20

  checkpoint:
    report_frequency: 5000  # episode
    save_frequency: 5000  # episode
    experiment_tag: 'dqn-permute-3'
    load_pretrained: False
    load_from_tag: 'checkpoints/dqn-permute-3/model'
    load_parameter_keywords: ["rgcns", "word_embedding", "node_embedding", "relation_embedding", "word_embedding_prj"]

  model:
    use_pretrained_embedding: True
    word_embedding_size: 300
    word_embedding_trainable: False
    node_embedding_size: 100
    node_embedding_trainable: True
    relation_embedding_size: 32
    relation_embedding_trainable: True
    embedding_dropout: 0.
    encoder_layers: 1
    decoder_layers: 1
    action_scorer_layers: 1
    encoder_conv_num: 5
    block_hidden_dim: 64
    n_heads: 1
    dropout: 0.
    attention_dropout: 0.
    block_dropout: 0.

rl:
  data_path: "rl.0.2"
  difficulty_level: 5  # 3/7/5/9 corresponds to Level 1/2/3/4 of the paper
  training_size: 100  # 1, 20, 100

  training:
    max_nb_steps_per_episode: 50  # after this many steps, a game is terminated
    learn_start_from_this_episode: 100
    target_net_update_frequency: 500  # sync target net with online net per this many epochs
    use_negative_reward: False
    step_penalty: 0.0
    update_n_iterations: 1
    sync_frequency: 4
    pipelines: 1

  evaluate:
    max_nb_steps_per_episode: 50

  replay:
    buffer_reward_threshold: 0.1  # a new sequence of transitions has to be k times better than average rewards in current replay buffer. Use -1 to disable.
    accumulate_reward_from_final: False
    prioritized_replay_beta: 0.4
    prioritized_replay_eps: 0.000001
    discount_gamma_game_reward: 0.9
    replay_memory_capacity: 500000  # adjust this depending on your RAM size
    replay_memory_priority_fraction: 0.6
    update_per_k_game_steps: 1
    replay_batch_size: 256
    multi_step: 3
    replay_sample_history_length: 8
    replay_sample_update_from: 4

  epsilon_greedy:
    noisy_net: False  # if this is true, then epsilon greedy is disabled
    epsilon_anneal_episodes: 200000  # -1 if not annealing
    epsilon_anneal_from: 1.0
    epsilon_anneal_to: 0.1

  model:
    enable_recurrent_memory: False
    enable_graph_input: False
    enable_text_input: True

  env:
    provide_history: False
    observation_format: "<sep> {PREV_ACT} <sep> {obs} "
    max_token: 256
    permute_entity: ./vocabularies/interchangeable-0.6.json
